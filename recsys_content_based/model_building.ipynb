{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building: Latent Dirichlet Allocation\n",
    "\n",
    "Description: \n",
    "\n",
    "In section 1) Model tuning (hyperparameter selection) - decide on the number of topics (n_components) to use in the model. Run 5-fold cross validation for n_components in range (2-30). Use perplexity calculated on the test set as the evaluation criteria\n",
    "\n",
    "In section 2) train full model with optimal number of topics determined from section 1.\n",
    "\n",
    "\n",
    "LDA code was adapted from: [scikit_learn](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py) - Authors: Olivier Grisel, Lars Buitinck, Chyi-Kwei Yau - License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Hyperparameter selection - determine number of topcs (n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in scipy sparse matrix\n",
    "X = sparse.load_npz(\"data_preprocessing_out\\\\X.npz\")\n",
    "\n",
    "with open(\"data_preprocessing_out\\\\word_key.txt\", \"rb\") as f:\n",
    "    word_key = pickle.load(f)\n",
    "\n",
    "n_movies, n_features = X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of documents for train/test data\n",
    "n_train_samples = 8000\n",
    "n_test_samples = 2000\n",
    "\n",
    "# hyperparameter selection - number of topics (n_components)\n",
    "n_folds = 5\n",
    "rand_seeds = [0, 11, 22, 33, 44]\n",
    "n_components = np.array(\n",
    "    [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30]\n",
    ")\n",
    "perp_train = np.zeros((n_components.shape[0], n_folds))\n",
    "perp_test = np.zeros((n_components.shape[0], n_folds))\n",
    "for k in range(n_folds):\n",
    "    # construct train/test split\n",
    "    print(f\"fold number {k}:\")\n",
    "    X_train, X_test = train_test_split(\n",
    "        X,\n",
    "        train_size=n_train_samples,\n",
    "        test_size=n_test_samples,\n",
    "        random_state=rand_seeds[k],\n",
    "    )\n",
    "    for j, ncomp in enumerate(n_components):\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=ncomp,  # x4 components 2.5x the time\n",
    "            max_iter=55,  # x2 iterations 2x the time\n",
    "            learning_decay=0.55,\n",
    "            learning_method=\"online\",\n",
    "            learning_offset=64.0,\n",
    "            batch_size=256,\n",
    "            random_state=0,\n",
    "            evaluate_every=5,\n",
    "            verbose=0,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        lda.fit(X_train)  # scales (roughly) linearly with number of training examples\n",
    "        perp_train[j, k] = np.round(lda.perplexity(X_train), 1)\n",
    "        perp_test[j, k] = np.round(lda.perplexity(X_test), 1)\n",
    "        if k == 0:\n",
    "            pickle.dump(\n",
    "                lda,\n",
    "                open(\n",
    "                    \"model_building\\\\hyperparameter_tuning\\\\model_\"\n",
    "                    + str(ncomp)\n",
    "                    + \"_components.sav\",\n",
    "                    \"wb\",\n",
    "                ),\n",
    "            )\n",
    "        print(\n",
    "            f\"n_components = {ncomp}: p_train = {perp_train[j, k]}, p_test = {perp_test[j, k]}\"\n",
    "        )\n",
    "\n",
    "with open(\"model_building\\\\perp_train.txt\", \"wb\") as f:\n",
    "    pickle.dump(perp_train, f)\n",
    "with open(\"model_building\\\\perp_test.txt\", \"wb\") as f:\n",
    "    pickle.dump(perp_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean and std pf perplexity on the train/test data\n",
    "perp_test_mean = np.mean(perp_test, axis=1)\n",
    "perp_test_std = np.std(perp_test, axis=1)\n",
    "\n",
    "perp_train_mean = np.mean(perp_train, axis=1)\n",
    "perp_train_std = np.std(perp_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean perplexity for train/test data as\n",
    "# a function of n_components\n",
    "\n",
    "plt.rc(\"font\", **{\"size\": 14})\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(\n",
    "    np.linspace(10, 30.5, 100),\n",
    "    np.mean(perp_test_mean[-4:]) * np.ones(shape=(100,)),\n",
    "    \"k--\",\n",
    ")\n",
    "plt.errorbar(\n",
    "    n_components,\n",
    "    perp_test_mean[:],\n",
    "    yerr=perp_test_std,\n",
    "    xerr=None,\n",
    "    fmt=\"r*\",\n",
    "    label=\"test\",\n",
    ")\n",
    "plt.errorbar(\n",
    "    n_components,\n",
    "    perp_train_mean[:],\n",
    "    yerr=perp_train_std,\n",
    "    xerr=None,\n",
    "    fmt=\"b*\",\n",
    "    label=\"train\",\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"perplexity\")\n",
    "ax.set_xlabel(\"n_components\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Fit full model with predetermined n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model with optimal value for n_components\n",
    "\n",
    "# model inputs\n",
    "n_samples = 12000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "\n",
    "# construct train/test split\n",
    "X_train, X_test = train_test_split(\n",
    "    X, train_size=n_samples, test_size=1000, random_state=22\n",
    ")\n",
    "\n",
    "# took 90 minutes (~1.75 minutes/iteration) on 10,000 words, 20 components, 10177 randomly chosen samples, 52 iterations, perplexity dropped <0.1/iteration at 52th iteration\n",
    "print(\n",
    "    f\"Fitting LDA models with tf features, n_samples={n_samples} and n_features={n_features}...\"\n",
    ")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,  # x4 components 2.5x the time\n",
    "    max_iter=30,  # x2 iterations 2x the time\n",
    "    learning_decay=0.55,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=64.0,\n",
    "    batch_size=256,\n",
    "    random_state=0,\n",
    "    evaluate_every=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "t0 = time.time()\n",
    "lda.fit(X_train)  # scales (roughly) linearly with number of training examples\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "\n",
    "print(lda.perplexity(X_test))\n",
    "\n",
    "# predict topic weights for all scripts in the database (4 minutes for the entire corpus)\n",
    "# this will be precomputed and saved to be used for recommender app predicitons\n",
    "Xtran = lda.transform(X)\n",
    "with open(\"model_building\\\\Xtran.txt\", \"wb\") as f:\n",
    "    pickle.dump(Xtran, f)\n",
    "pickle.dump(lda, open(\"model_building\\\\model_2023_08_24.sav\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys_movie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
