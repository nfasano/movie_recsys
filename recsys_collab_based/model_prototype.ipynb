{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking evaluation with offline metrics: \n",
    "### implemented precision@k, recal@k, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import time\n",
    "\n",
    "# import gradio as gr\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import functions from surprise library\n",
    "\n",
    "from surprise import SVD, CTM\n",
    "from surprise import Dataset, NormalPredictor, Reader\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "from surprise.accuracy import rmse\n",
    "\n",
    "import line_profiler\n",
    "%load_ext line_profiler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTM(AlgoBase):\n",
    "    \"\"\"Collaborative topic model using weights from Latent Dirichlet Allocation \n",
    "       to inform the learned item representation from SVD.\n",
    "\n",
    "    The prediction :math:`\\\\hat{r}_{ui}` is set as:\n",
    "\n",
    "    .. math::\n",
    "        \\\\hat{r}_{ui} = \\\\mu + b_u + b_i + q_i^Tp_u\n",
    "\n",
    "    If user :math:`u` is unknown, then the bias :math:`b_u` and the factors\n",
    "    :math:`p_u` are assumed to be zero. The same applies for item :math:`i`\n",
    "    with :math:`b_i` and :math:`q_i`.\n",
    "\n",
    "    For details, see equation (5) from :cite:`Koren:2009`. See also\n",
    "    :cite:`Ricci:2010`, section 5.3.1.\n",
    "\n",
    "    To estimate all the unknown, we minimize the following regularized squared\n",
    "    error:\n",
    "\n",
    "    .. math::\n",
    "        \\\\sum_{r_{ui} \\\\in R_{train}} \\\\left(r_{ui} - \\\\hat{r}_{ui} \\\\right)^2 +\n",
    "        \\\\lambda\\\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\\\\right)\n",
    "\n",
    "\n",
    "    The minimization is performed by a very straightforward stochastic gradient\n",
    "    descent:\n",
    "\n",
    "    .. math::\n",
    "        b_u &\\\\leftarrow b_u &+ \\\\gamma (e_{ui} - \\\\lambda b_u)\\\\\\\\\n",
    "        b_i &\\\\leftarrow b_i &+ \\\\gamma (e_{ui} - \\\\lambda b_i)\\\\\\\\\n",
    "        p_u &\\\\leftarrow p_u &+ \\\\gamma (e_{ui} \\\\cdot q_i - \\\\lambda p_u)\\\\\\\\\n",
    "        q_i &\\\\leftarrow q_i &+ \\\\gamma (e_{ui} \\\\cdot p_u - \\\\lambda q_i)\n",
    "\n",
    "    where :math:`e_{ui} = r_{ui} - \\\\hat{r}_{ui}`. These steps are performed\n",
    "    over all the ratings of the trainset and repeated ``n_epochs`` times.\n",
    "    Baselines are initialized to ``0``. User and item factors are randomly\n",
    "    initialized according to a normal distribution, which can be tuned using\n",
    "    the ``init_mean`` and ``init_std_dev`` parameters.\n",
    "\n",
    "    You also have control over the learning rate :math:`\\\\gamma` and the\n",
    "    regularization term :math:`\\\\lambda`. Both can be different for each\n",
    "    kind of parameter (see below). By default, learning rates are set to\n",
    "    ``0.005`` and regularization terms are set to ``0.02``.\n",
    "\n",
    "    .. _unbiased_note:\n",
    "\n",
    "    .. note::\n",
    "        You can choose to use an unbiased version of this algorithm, simply\n",
    "        predicting:\n",
    "\n",
    "        .. math::\n",
    "            \\\\hat{r}_{ui} = q_i^Tp_u\n",
    "\n",
    "        This is equivalent to Probabilistic Matrix Factorization\n",
    "        (:cite:`salakhutdinov2008a`, section 2) and can be achieved by setting\n",
    "        the ``biased`` parameter to ``False``.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        theta: The topic representations from LDA (numpy array of size (n_itmes x n_factors))\n",
    "        n_factors: The number of factors. Must equal the number of topics LDA.\n",
    "        n_epochs: The number of iteration of the SGD procedure. Default is\n",
    "            ``20``.\n",
    "        biased(bool): Whether to use baselines (or biases). See :ref:`note\n",
    "            <unbiased_note>` above.  Default is ``True``.\n",
    "        init_mean: The mean of the normal distribution for factor vectors\n",
    "            initialization. Default is ``0``.\n",
    "        init_std_dev: The standard deviation of the normal distribution for\n",
    "            factor vectors initialization. Default is ``0.1``.\n",
    "        lr_all: The learning rate for all parameters. Default is ``0.005``.\n",
    "        reg_all: The regularization term for all parameters. Default is\n",
    "            ``0.02``.\n",
    "        lr_bu: The learning rate for :math:`b_u`. Takes precedence over\n",
    "            ``lr_all`` if set. Default is ``None``.\n",
    "        lr_bi: The learning rate for :math:`b_i`. Takes precedence over\n",
    "            ``lr_all`` if set. Default is ``None``.\n",
    "        lr_pu: The learning rate for :math:`p_u`. Takes precedence over\n",
    "            ``lr_all`` if set. Default is ``None``.\n",
    "        lr_qi: The learning rate for :math:`q_i`. Takes precedence over\n",
    "            ``lr_all`` if set. Default is ``None``.\n",
    "        reg_bu: The regularization term for :math:`b_u`. Takes precedence\n",
    "            over ``reg_all`` if set. Default is ``None``.\n",
    "        reg_bi: The regularization term for :math:`b_i`. Takes precedence\n",
    "            over ``reg_all`` if set. Default is ``None``.\n",
    "        reg_pu: The regularization term for :math:`p_u`. Takes precedence\n",
    "            over ``reg_all`` if set. Default is ``None``.\n",
    "        reg_qi: The regularization term for :math:`q_i`. Takes precedence\n",
    "            over ``reg_all`` if set. Default is ``None``.\n",
    "        random_state(int, RandomState instance from numpy, or ``None``):\n",
    "            Determines the RNG that will be used for initialization. If\n",
    "            int, ``random_state`` will be used as a seed for a new RNG. This is\n",
    "            useful to get the same initialization over multiple calls to\n",
    "            ``fit()``.  If RandomState instance, this same instance is used as\n",
    "            RNG. If ``None``, the current RNG from numpy is used.  Default is\n",
    "            ``None``.\n",
    "        verbose: If ``True``, prints the current epoch. Default is ``False``.\n",
    "\n",
    "    Attributes:\n",
    "        pu(numpy array of size (n_users, n_factors)): The user factors (only\n",
    "            exists if ``fit()`` has been called)\n",
    "        qi(numpy array of size (n_items, n_factors)): The item factors (only\n",
    "            exists if ``fit()`` has been called)\n",
    "        theta(numpy array of size (n_items, n_factors)): The item topic factors (only\n",
    "            exists if ``fit()`` has been called)\n",
    "        bu(numpy array of size (n_users)): The user biases (only\n",
    "            exists if ``fit()`` has been called)\n",
    "        bi(numpy array of size (n_items)): The item biases (only\n",
    "            exists if ``fit()`` has been called)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_factors=100, n_epochs=20, biased=True, init_mean=0,\n",
    "                 init_std_dev=.1, lr_all=.005,\n",
    "                 reg_all=.02, lr_bu=None, lr_bi=None, lr_pu=None, lr_qi=None,\n",
    "                 reg_bu=None, reg_bi=None, reg_pu=None, reg_qi=None,\n",
    "                 random_state=None, verbose=False):\n",
    "\n",
    "        self.n_factors = n_factors\n",
    "        self.n_epochs = n_epochs\n",
    "        self.biased = biased\n",
    "        self.init_mean = init_mean\n",
    "        self.init_std_dev = init_std_dev\n",
    "        self.lr_bu = lr_bu if lr_bu is not None else lr_all\n",
    "        self.lr_bi = lr_bi if lr_bi is not None else lr_all\n",
    "        self.lr_pu = lr_pu if lr_pu is not None else lr_all\n",
    "        self.lr_qi = lr_qi if lr_qi is not None else lr_all\n",
    "        self.reg_bu = reg_bu if reg_bu is not None else reg_all\n",
    "        self.reg_bi = reg_bi if reg_bi is not None else reg_all\n",
    "        self.reg_pu = reg_pu if reg_pu is not None else reg_all\n",
    "        self.reg_qi = reg_qi if reg_qi is not None else reg_all\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "    def fit(self, trainset):\n",
    "\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        self.sgd(trainset)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def sgd(self, trainset):\n",
    "\n",
    "        # OK, let's breath. I've seen so many different implementation of this\n",
    "        # algorithm that I just not sure anymore of what it should do. I've\n",
    "        # implemented the version as described in the BellKor papers (RS\n",
    "        # Handbook, etc.). Mymedialite also does it this way. In his post\n",
    "        # however, Funk seems to implicitly say that the algo looks like this\n",
    "        # (see reg below):\n",
    "        # for f in range(n_factors):\n",
    "        #       for _ in range(n_iter):\n",
    "        #           for u, i, r in all_ratings:\n",
    "        #               err = r_ui - <p[u, :f+1], q[i, :f+1]>\n",
    "        #               update p[u, f]\n",
    "        #               update q[i, f]\n",
    "        # which is also the way https://github.com/aaw/IncrementalSVD.jl\n",
    "        # implemented it.\n",
    "        #\n",
    "        # Funk: \"Anyway, this will train one feature (aspect), and in\n",
    "        # particular will find the most prominent feature remaining (the one\n",
    "        # that will most reduce the error that's left over after previously\n",
    "        # trained features have done their best). When it's as good as it's\n",
    "        # going to get, shift it onto the pile of done features, and start a\n",
    "        # new one. For efficiency's sake, cache the residuals (all 100 million\n",
    "        # of them) so when you're training feature 72 you don't have to wait\n",
    "        # for predictRating() to re-compute the contributions of the previous\n",
    "        # 71 features. You will need 2 Gig of ram, a C compiler, and good\n",
    "        # programming habits to do this.\"\n",
    "\n",
    "        # A note on cythonization: I haven't dived into the details, but\n",
    "        # accessing 2D arrays like pu using just one of the indices like pu[u]\n",
    "        # is not efficient. That's why the old (cleaner) version can't be used\n",
    "        # anymore, we need to compute the dot products by hand, and update\n",
    "        # user and items factors by iterating over all factors...\n",
    "\n",
    "        rng = get_rng(self.random_state)\n",
    "\n",
    "        # user biases\n",
    "        cdef double [::1] bu = np.zeros(trainset.n_users, dtype=np.double)\n",
    "        # item biases\n",
    "        cdef double [::1] bi = np.zeros(trainset.n_items, dtype=np.double)\n",
    "        # user factors\n",
    "        cdef double [:, ::1] pu = rng.normal(self.init_mean, self.init_std_dev, size=(trainset.n_users, self.n_factors))\n",
    "        # item factors\n",
    "        cdef double [:, ::1] qi = rng.normal(self.init_mean, self.init_std_dev, size=(trainset.n_items, self.n_factors))\n",
    "\n",
    "        cdef int u, i, f\n",
    "        cdef int n_factors = self.n_factors\n",
    "        cdef bint biased = self.biased\n",
    "\n",
    "        cdef double r, err, dot, puf, qif, thetaif\n",
    "        cdef double global_mean = self.trainset.global_mean\n",
    "\n",
    "        cdef double lr_bu = self.lr_bu\n",
    "        cdef double lr_bi = self.lr_bi\n",
    "        cdef double lr_pu = self.lr_pu\n",
    "        cdef double lr_qi = self.lr_qi\n",
    "\n",
    "        cdef double reg_bu = self.reg_bu\n",
    "        cdef double reg_bi = self.reg_bi\n",
    "        cdef double reg_pu = self.reg_pu\n",
    "        cdef double reg_qi = self.reg_qi\n",
    "\n",
    "        if not biased:\n",
    "            global_mean = 0\n",
    "\n",
    "        for current_epoch in range(self.n_epochs):\n",
    "            if self.verbose:\n",
    "                print(\"Processing epoch {}\".format(current_epoch))\n",
    "\n",
    "            for u, i, r in trainset.all_ratings():\n",
    "                # compute current error\n",
    "                dot = 0  # <q_i, p_u>\n",
    "                for f in range(n_factors):\n",
    "                    dot += qi[i, f] * pu[u, f]\n",
    "                err = r - (global_mean + bu[u] + bi[i] + dot)\n",
    "\n",
    "                # update biases\n",
    "                if biased:\n",
    "                    bu[u] += lr_bu * (err - reg_bu * bu[u])\n",
    "                    bi[i] += lr_bi * (err - reg_bi * bi[i])\n",
    "\n",
    "                # update factors\n",
    "                for f in range(n_factors):\n",
    "                    puf = pu[u, f]\n",
    "                    qif = qi[i, f]\n",
    "                    thetaif = theta[i, f]\n",
    "                    pu[u, f] += lr_pu * (err * qif - reg_pu * puf)\n",
    "                    qi[i, f] += lr_qi * (err * puf - reg_qi * (qif - thetaif))\n",
    "\n",
    "        self.bu = np.asarray(bu)\n",
    "        self.bi = np.asarray(bi)\n",
    "        self.pu = np.asarray(pu)\n",
    "        self.qi = np.asarray(qi)\n",
    "        self.theta = np.asarray(theta)\n",
    "    \n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        # Should we cythonize this as well?\n",
    "\n",
    "        known_user = self.trainset.knows_user(u)\n",
    "        known_item = self.trainset.knows_item(i)\n",
    "\n",
    "        if self.biased:\n",
    "            est = self.trainset.global_mean\n",
    "\n",
    "            if known_user:\n",
    "                est += self.bu[u]\n",
    "\n",
    "            if known_item:\n",
    "                est += self.bi[i]\n",
    "\n",
    "            if known_user and known_item:\n",
    "                est += np.dot(self.qi[i], self.pu[u])\n",
    "\n",
    "        else:\n",
    "            if known_user and known_item:\n",
    "                est = np.dot(self.qi[i], self.pu[u])\n",
    "            else:\n",
    "                raise PredictionImpossible('User and item are unknown.')\n",
    "\n",
    "        return est\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tuned model and transformed document-topic matrix\n",
    "lda_main = pickle.load(open('..\\\\recsys_content_based\\\\model_building_out\\\\model_2023_08_16.sav', 'rb'))\n",
    "\n",
    "with open(\"..\\\\recsys_content_based\\\\data_preprocessing_out\\\\word_key.txt\", \"rb\") as f:\n",
    "    word_key = pickle.load(f)\n",
    "\n",
    "# read in movie database\n",
    "df = pd.read_csv(\"..\\\\database\\\\dataset_spaces_upload.csv\", index_col=[0])\n",
    "\n",
    "# read in scipy sparse matrix\n",
    "X = sparse.load_npz(\"..\\\\recsys_content_based\\\\data_preprocessing_out\\\\X.npz\")\n",
    "with open(\"..\\\\recsys_content_based\\\\model_building_out\\\\Xtran.txt\", \"rb\") as f:\n",
    "    Xtran_main = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('..\\\\database\\\\dataset_film_scripts\\\\springfield_movie_scripts_2023_01_13_clean.csv', index_col = [0])\n",
    "df_orig = df_orig.drop(['script_text', 'springfield_link', 'tmdb_poster_link', 'imdb_link'], axis=1)\n",
    "df_orig['recsys_id'] = df_orig.index\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movielens = pd.read_csv('..\\\\database\\\\dataset_movieLens\\\\links.csv')\n",
    "# df_movielens['movielens_id'] = df_movielens.index\n",
    "df_movielens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_orig.join(df_movielens.dropna().set_index('tmdbId'), how='left', on='tmdb_id')\n",
    "df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second option is to join on imdbId -- both options yield the same result ~ 20,300 non-null matches\n",
    "# df_joined = df_orig.join(df_movielens.set_index('imdbId'), how='left', on='imdb_id')\n",
    "# df_joined.head()\n",
    "# df_joined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movielens_ratings = pd.read_csv('..\\\\database\\\\dataset_movieLens\\\\ratings.csv')\n",
    "df_movielens_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out movies from ratings matrix that are not in script database\n",
    "unique_movielens_ids = df_joined['movieId'].unique()[1:]\n",
    "unique_movielens_ids = np.sort(unique_movielens_ids.astype(int))\n",
    "movieId = np.array(df_movielens_ratings['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_mask = [True if j in unique_movielens_ids else False for j in movieId]\n",
    "df_movielens_ratings = df_movielens_ratings.loc[bool_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all users from ratings matrix that rated less than 6 films\n",
    "unique_movielens_users = np.array(df_movielens_ratings['userId'].value_counts().index)\n",
    "num_ratings_per_user = np.array(df_movielens_ratings['userId'].value_counts())\n",
    "userId = np.array(df_movielens_ratings['userId'])\n",
    "\n",
    "users_drop = unique_movielens_users[num_ratings_per_user <= 5]\n",
    "bool_mask = [False if j in users_drop else True for j in userId]\n",
    "df_movielens_ratings = df_movielens_ratings.loc[bool_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop movies from Xtran_main and df that are not in movie lens database\n",
    "df_joined = df_joined.drop_duplicates(subset='tmdb_id')\n",
    "df_joined = df_joined.dropna(subset='movieId')\n",
    "\n",
    "Xtran_main = Xtran_main[df_joined.index,:]\n",
    "df = df.loc[df_joined.index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_movielens_ratings.join(df_joined.set_index(\"movieId\"), how=\"left\", on=\"movieId\")\n",
    "\n",
    "unique_users = np.array(df_final['userId'].value_counts().sort_index().index)\n",
    "num_ratings_per_user = np.array(df_final['userId'].value_counts().sort_index())\n",
    "diff = [np.sum(num_ratings_per_user[:j])  if j > 0 else 0 for j in range(len(unique_users))]\n",
    "\n",
    "df_imdb_sort = df.copy()\n",
    "# df_imdb_sort['script_id'] = df_imdb_sort.index\n",
    "# df_imdb_sort = df_imdb_sort.set_index('imdb_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form training and testing dataframes\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_list_sort = list(df_imdb_sort['imdb_id'])\n",
    "df.columns = [\"Title\", \"Year\", \"Genres\", \"IMDb Rating\", 'num_votes', 'is_adult', 'imdb_id', 'imdb_link', 'tmdb_poster_link']\n",
    "df = df[[\"Title\", \"Year\", \"IMDb Rating\", \"Genres\"]]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_matrix = df_final[['userId', 'movieId', 'rating']].copy()\n",
    "df_ratings_matrix['rating'].unique()\n",
    "\n",
    "df_ratings_matrix = df_ratings_matrix.iloc[0:100_000].copy()\n",
    "\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(df_ratings_matrix[[\"userId\", \"movieId\", \"rating\"]], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the movielens-100k dataset (download it if needed).\n",
    "# data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Use the famous SVD algorithm.\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "algo = SVD(n_factors=20, n_epochs=20)\n",
    "\n",
    "algo.fit(trainset=trainset)\n",
    "\n",
    "rmse(algo.test(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iids_in_train_set = [trainset.to_raw_iid(j) for j in range(trainset.n_items)]\n",
    "iids_in_train_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys_movie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
