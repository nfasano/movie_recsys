{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a CTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import time\n",
    "\n",
    "# import gradio as gr\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import functions from surprise library\n",
    "\n",
    "from surprise import SVD, CTM\n",
    "from surprise import Dataset, NormalPredictor, Reader\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "from surprise.accuracy import rmse\n",
    "\n",
    "import line_profiler\n",
    "%load_ext line_profiler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tuned model and transformed document-topic matrix\n",
    "lda_main = pickle.load(open('..\\\\recsys_content_based\\\\model_building_out\\\\model_2023_08_16.sav', 'rb'))\n",
    "\n",
    "with open(\"..\\\\recsys_content_based\\\\data_preprocessing_out\\\\word_key.txt\", \"rb\") as f:\n",
    "    word_key = pickle.load(f)\n",
    "\n",
    "# read in movie database\n",
    "df = pd.read_csv(\"..\\\\database\\\\dataset_spaces_upload.csv\", index_col=[0])\n",
    "\n",
    "# read in scipy sparse matrix\n",
    "X = sparse.load_npz(\"..\\\\recsys_content_based\\\\data_preprocessing_out\\\\X.npz\")\n",
    "with open(\"..\\\\recsys_content_based\\\\model_building_out\\\\Xtran.txt\", \"rb\") as f:\n",
    "    Xtran_main = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in cleaned movie scripts dataset\n",
    "df_orig = pd.read_csv('..\\\\database\\\\dataset_film_scripts\\\\springfield_movie_scripts_2023_01_13_clean.csv', index_col = [0])\n",
    "df_orig = df_orig.drop(['script_text', 'springfield_link', 'tmdb_poster_link', 'imdb_link'], axis=1)\n",
    "df_orig['recsys_id'] = df_orig.index\n",
    "print(df_orig.info())\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in movielens dataset\n",
    "df_movielens = pd.read_csv('..\\\\database\\\\dataset_movieLens\\\\links.csv')\n",
    "df_movielens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movielens_ratings = pd.read_csv('..\\\\database\\\\dataset_movieLens\\\\ratings.csv')\n",
    "df_movielens_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join movie lens database with scripts dataset\n",
    "df_joined = df_orig.join(df_movielens.dropna().set_index('tmdbId'), how='left', on='tmdb_id')\n",
    "\n",
    "# drop duplicates and missing movieIds\n",
    "df_joined = df_joined.drop_duplicates(subset='tmdb_id')\n",
    "df_joined = df_joined.dropna(subset='movieId')\n",
    "print(df_joined.info())\n",
    "df_joined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second option is to join on imdbId -- both options yield the same result ~ 20,300 non-null matches\n",
    "# df_joined = df_orig.join(df_movielens.set_index('imdbId'), how='left', on='imdb_id')\n",
    "# df_joined.head()\n",
    "# df_joined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out movies from ratings matrix that are not in script database\n",
    "# takes about 5 minutes to run\n",
    "unique_movielens_ids = df_joined['movieId'].unique()[1:]\n",
    "unique_movielens_ids = np.sort(unique_movielens_ids.astype(int))\n",
    "movieId = np.array(df_movielens_ratings['movieId'])\n",
    "\n",
    "bool_mask = [True if j in unique_movielens_ids else False for j in movieId]\n",
    "df_movielens_ratings = df_movielens_ratings.loc[bool_mask]\n",
    "\n",
    "# drop all users from ratings matrix that rated less than 6 films\n",
    "unique_movielens_users = np.array(df_movielens_ratings['userId'].value_counts().index)\n",
    "num_ratings_per_user = np.array(df_movielens_ratings['userId'].value_counts())\n",
    "userId = np.array(df_movielens_ratings['userId'])\n",
    "\n",
    "users_drop = unique_movielens_users[num_ratings_per_user <= 5]\n",
    "bool_mask = [False if j in users_drop else True for j in userId]\n",
    "df_movielens_ratings = df_movielens_ratings.loc[bool_mask]\n",
    "\n",
    "\n",
    "df_movielens_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_movielens_ratings.join(df_joined.set_index(\"movieId\"), how=\"left\", on=\"movieId\")\n",
    "df_final = df_final.dropna()\n",
    "df_final['recsys_id'] = df_final['recsys_id'].astype('int')\n",
    "# unique_users = np.array(df_final['userId'].value_counts().sort_index().index)\n",
    "# num_ratings_per_user = np.array(df_final['userId'].value_counts().sort_index())\n",
    "# diff = [np.sum(num_ratings_per_user[:j])  if j > 0 else 0 for j in range(len(unique_users))]\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop movies from Xtran_main and df that are not in movie lens database\n",
    "jkeep = sorted(df_final['recsys_id'].astype('int').unique().tolist())\n",
    "# Xtran_main = Xtran_main[jkeep,:]\n",
    "df = df.loc[jkeep].reset_index(drop=True)\n",
    "df_orig = df_orig.loc[jkeep]\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form dataset for SVD algorithm\n",
    "df_ratings_matrix = df_final[['userId', 'recsys_id', 'rating']].copy()\n",
    "\n",
    "df_ratings_matrix = df_ratings_matrix.iloc[0:100_000].copy()\n",
    "\n",
    "\n",
    "# map half ratings to integer ratings using a 50/50 split to nearest whole number\n",
    "df_ratings_matrix[df_ratings_matrix['rating'] == 0.5] = 1\n",
    "for jrating in [1.5,2.5,3.5,4.5]:\n",
    "    a = np.array(df_ratings_matrix[df_ratings_matrix['rating'] == jrating].index,dtype=int)\n",
    "    np.random.shuffle(a) # shuffle in-place, returns none\n",
    "    num_ratings = len(a)\n",
    "    df_ratings_matrix.loc[list(a[0:int(np.ceil(num_ratings/2))]),'rating'] = jrating-.5\n",
    "    df_ratings_matrix.loc[list(a[int(np.ceil(num_ratings/2)):]),'rating'] = jrating+.5\n",
    "\n",
    "\n",
    "reader = Reader(rating_scale=(1.0, 5.0))\n",
    "data = Dataset.load_from_df(df_ratings_matrix[[\"userId\", \"recsys_id\", \"rating\"]], reader)\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iids_in_train_set = [trainset.to_raw_iid(j) for j in range(trainset.n_items)]\n",
    "theta = Xtran_main[iids_in_train_set, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = [0]\n",
    "for k in hp:\n",
    "    algoC = CTM(n_factors=20, n_epochs=20, theta=theta*k)\n",
    "\n",
    "    algoC.fit(trainset=trainset)\n",
    "    rmse(algoC.test(testset))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVD(n_factors=20, n_epochs=20, verbose=True)\n",
    "algo.fit(trainset=trainset)\n",
    "rmse(algo.test(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate precision@k and recall@k metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = algoC.test(testset)\n",
    "precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=3.5)\n",
    "\n",
    "# Precision and recall can then be averaged over all users\n",
    "print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = algo.test(testset)\n",
    "precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=3.5)\n",
    "\n",
    "# Precision and recall can then be averaged over all users\n",
    "print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys_movie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
